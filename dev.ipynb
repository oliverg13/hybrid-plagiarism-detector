{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Local imports\n",
    "from utils.general import printt, print_sucess, N_PROCESSES\n",
    "from utils.data_load_extract import (\n",
    "    ensure_directory_exists_and_is_empty,\n",
    "    extract_sources_from_suspicious_xml,\n",
    "    divide_df_sentences\n",
    "    )\n",
    "from utils.preprocessing import preprocessing_data\n",
    "from utils.filtering import document_filtering, sentence_filtering\n",
    "from utils.metrics import similarity_computation, evaluate_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_sources = False\n",
    "reload_suspicious = False\n",
    "results_filename = \"data_for_figure_20.txt\"\n",
    "SOURCE_LIMIT = 64\n",
    "SUSPICIOUS_LIMIT = 8\n",
    "THRESHOLD_LIST = [0.65] # [0.4, 0.45, 0.5, 0.55, 0.60, 0.65, 0.70]\n",
    "BETA = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:37:18] Starting the algorithm...\n",
      "[17:37:18] N_PROCESSES: 6\n"
     ]
    }
   ],
   "source": [
    "# 1. Initial setup (paths and number of processes for parallelization)\n",
    "printt(\"Starting the algorithm...\")\n",
    "# Getting the current directory path and setting up the base path for data.\n",
    "current_dir = os.getcwd()\n",
    "base_data_path = os.path.join(\n",
    "    current_dir, \"pan-plagiarism-corpus-2011/external-detection-corpus\"\n",
    ")\n",
    "# Print the number of processes that will be used for parallel operations.\n",
    "printt(f\"N_PROCESSES: {N_PROCESSES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:37:19] Setting up directories...\n"
     ]
    }
   ],
   "source": [
    "# 2. Directory setup\n",
    "printt(\"Setting up directories...\")\n",
    "# Initialize the directories for raw and cleaned versions of both source and suspicious documents.\n",
    "# Set up paths for raw and cleaned source documents.\n",
    "cleaned_source_path = os.path.join(current_dir, \"cleaned-source-documents\")\n",
    "source_path = os.path.join(base_data_path, \"source-document\")\n",
    "# Set up paths for raw and cleaned suspicious documents.\n",
    "cleaned_suspicious_path = os.path.join(current_dir, \"cleaned-suspicious-documents\")\n",
    "suspicious_path = os.path.join(base_data_path, \"suspicious-document\")\n",
    "if reload_sources:\n",
    "    ensure_directory_exists_and_is_empty(cleaned_source_path)\n",
    "if reload_suspicious:\n",
    "    ensure_directory_exists_and_is_empty(cleaned_suspicious_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:37:21] Extracting reference results...\n"
     ]
    }
   ],
   "source": [
    "# 3. Extract real results from XML files to understand the ground truth of plagiarism references\n",
    "printt(\"Extracting reference results...\")\n",
    "# Extract and store the plagiarism references from the suspicious XML files\n",
    "df_plagiarism_references = extract_sources_from_suspicious_xml(suspicious_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suspicious_filename</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>suspicious_offset</th>\n",
       "      <th>suspicious_length</th>\n",
       "      <th>source_offset</th>\n",
       "      <th>source_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>suspicious-document00185.txt</td>\n",
       "      <td>source-document03291.txt</td>\n",
       "      <td>2361</td>\n",
       "      <td>3423</td>\n",
       "      <td>23508</td>\n",
       "      <td>3525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suspicious-document00185.txt</td>\n",
       "      <td>source-document03291.txt</td>\n",
       "      <td>47733</td>\n",
       "      <td>1967</td>\n",
       "      <td>2554</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suspicious-document00185.txt</td>\n",
       "      <td>source-document03291.txt</td>\n",
       "      <td>131081</td>\n",
       "      <td>12809</td>\n",
       "      <td>28727</td>\n",
       "      <td>19497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suspicious-document00185.txt</td>\n",
       "      <td>source-document03291.txt</td>\n",
       "      <td>252530</td>\n",
       "      <td>3956</td>\n",
       "      <td>8883</td>\n",
       "      <td>3975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suspicious-document00185.txt</td>\n",
       "      <td>source-document03291.txt</td>\n",
       "      <td>288473</td>\n",
       "      <td>801</td>\n",
       "      <td>1587</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            suspicious_filename           source_filename  suspicious_offset  \\\n",
       "0  suspicious-document00185.txt  source-document03291.txt               2361   \n",
       "1  suspicious-document00185.txt  source-document03291.txt              47733   \n",
       "2  suspicious-document00185.txt  source-document03291.txt             131081   \n",
       "3  suspicious-document00185.txt  source-document03291.txt             252530   \n",
       "4  suspicious-document00185.txt  source-document03291.txt             288473   \n",
       "\n",
       "   suspicious_length  source_offset  source_length  \n",
       "0               3423          23508           3525  \n",
       "1               1967           2554           1976  \n",
       "2              12809          28727          19497  \n",
       "3               3956           8883           3975  \n",
       "4                801           1587            820  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plagiarism_references.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:37:25] Preprocessing data...\n",
      "[17:37:25] Source documents preprocessing...\n",
      "[17:37:25] Suspicious documents preprocessing...\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Preprocess the raw data to prepare it for the subsequent analysis\n",
    "printt(\"Preprocessing data...\")\n",
    "printt(\"Source documents preprocessing...\")\n",
    "if reload_sources:\n",
    "    preprocessing_data(source_path, cleaned_source_path, SOURCE_LIMIT)\n",
    "printt(\"Suspicious documents preprocessing...\")\n",
    "if reload_suspicious:\n",
    "    preprocessing_data(suspicious_path, cleaned_suspicious_path, SUSPICIOUS_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>length</th>\n",
       "      <th>offset</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿Hier und dort waren die Stümpfe frisch abgesä...</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>hier und dort waren die stümpfe frisch abgesäg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bald schlossen sich die Stämme dichter zusamme...</td>\n",
       "      <td>82</td>\n",
       "      <td>136</td>\n",
       "      <td>bald schlossen sich die stämme dichter zusamme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Es schien auch, als ob die Bäume höher würden ...</td>\n",
       "      <td>104</td>\n",
       "      <td>219</td>\n",
       "      <td>e schien auch al ob die bäume höher würden und...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spinnfäden, die sich\\nvon Stamm zu Stamm spann...</td>\n",
       "      <td>164</td>\n",
       "      <td>353</td>\n",
       "      <td>spinnfäden die sichvon stamm zu stamm spannten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Erschöpft legte sich der Knabe unter einigen\\n...</td>\n",
       "      <td>68</td>\n",
       "      <td>518</td>\n",
       "      <td>erschöpft legte sich der knabe unter einigenta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  length  offset  \\\n",
       "0  ﻿Hier und dort waren die Stümpfe frisch abgesä...     135       0   \n",
       "1  Bald schlossen sich die Stämme dichter zusamme...      82     136   \n",
       "2  Es schien auch, als ob die Bäume höher würden ...     104     219   \n",
       "3  Spinnfäden, die sich\\nvon Stamm zu Stamm spann...     164     353   \n",
       "4  Erschöpft legte sich der Knabe unter einigen\\n...      68     518   \n",
       "\n",
       "                                    cleaned_sentence  \n",
       "0  hier und dort waren die stümpfe frisch abgesäg...  \n",
       "1  bald schlossen sich die stämme dichter zusamme...  \n",
       "2  e schien auch al ob die bäume höher würden und...  \n",
       "3  spinnfäden die sichvon stamm zu stamm spannten...  \n",
       "4  erschöpft legte sich der knabe unter einigenta...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_temp = pd.read_parquet(\"/Users/oliverg271828/Documents/paper-replication-reloaded/hybrid-plagiarism-detector/cleaned-source-documents/source-document00008.parquet\").head(5)\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:37:29] Filtering documents...\n",
      "[17:37:35] Starting common_tokens_matrix multiplication \n",
      "[17:37:35] Start filter index location of documents\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Identify potential source documents for each suspicious document\n",
    "printt(\"Filtering documents...\")\n",
    "suspicious_to_sources = document_filtering(cleaned_suspicious_path, cleaned_source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'suspicious-document00285.parquet': ['source-document00182.parquet', 'source-document00357.parquet', 'source-document00140.parquet', 'source-document00395.parquet', 'source-document00141.parquet', 'source-document00394.parquet', 'source-document00419.parquet', 'source-document00426.parquet', 'source-document00022.parquet', 'source-document00355.parquet', 'source-document00180.parquet', 'source-document00425.parquet', 'source-document00234.parquet', 'source-document00168.parquet', 'source-document00382.parquet', 'source-document00157.parquet', 'source-document00009.parquet', 'source-document00036.parquet', 'source-document00195.parquet', 'source-document00431.parquet', 'source-document00037.parquet', 'source-document00194.parquet', 'source-document00220.parquet', 'source-document00433.parquet', 'source-document00196.parquet', 'source-document00035.parquet', 'source-document00154.parquet', 'source-document00222.parquet', 'source-document00381.parquet', 'source-document00432.parquet', 'source-document00034.parquet'], 'suspicious-document00050.parquet': ['source-document00182.parquet', 'source-document00357.parquet', 'source-document00140.parquet', 'source-document00395.parquet', 'source-document00394.parquet', 'source-document00419.parquet', 'source-document00022.parquet', 'source-document00355.parquet', 'source-document00180.parquet', 'source-document00425.parquet', 'source-document00234.parquet', 'source-document00168.parquet', 'source-document00382.parquet', 'source-document00157.parquet', 'source-document00009.parquet', 'source-document00036.parquet', 'source-document00195.parquet', 'source-document00431.parquet', 'source-document00037.parquet', 'source-document00194.parquet', 'source-document00433.parquet', 'source-document00196.parquet', 'source-document00035.parquet', 'source-document00154.parquet', 'source-document00222.parquet', 'source-document00432.parquet', 'source-document00034.parquet'], 'suspicious-document00246.parquet': ['source-document00182.parquet', 'source-document00357.parquet', 'source-document00140.parquet', 'source-document00395.parquet', 'source-document00141.parquet', 'source-document00394.parquet', 'source-document00419.parquet', 'source-document00022.parquet', 'source-document00355.parquet', 'source-document00180.parquet', 'source-document00425.parquet', 'source-document00234.parquet', 'source-document00168.parquet', 'source-document00382.parquet', 'source-document00157.parquet', 'source-document00009.parquet', 'source-document00036.parquet', 'source-document00195.parquet', 'source-document00431.parquet', 'source-document00037.parquet', 'source-document00194.parquet', 'source-document00220.parquet', 'source-document00433.parquet', 'source-document00196.parquet', 'source-document00035.parquet', 'source-document00154.parquet', 'source-document00222.parquet', 'source-document00381.parquet', 'source-document00432.parquet', 'source-document00034.parquet', 'source-document00342.parquet'], 'suspicious-document00093.parquet': ['source-document00357.parquet', 'source-document00425.parquet', 'source-document00222.parquet'], 'suspicious-document00291.parquet': ['source-document00182.parquet', 'source-document00357.parquet', 'source-document00140.parquet', 'source-document00395.parquet', 'source-document00141.parquet', 'source-document00394.parquet', 'source-document00419.parquet', 'source-document00022.parquet', 'source-document00355.parquet', 'source-document00180.parquet', 'source-document00425.parquet', 'source-document00234.parquet', 'source-document00168.parquet', 'source-document00382.parquet', 'source-document00157.parquet', 'source-document00009.parquet', 'source-document00036.parquet', 'source-document00195.parquet', 'source-document00431.parquet', 'source-document00037.parquet', 'source-document00194.parquet', 'source-document00433.parquet', 'source-document00196.parquet', 'source-document00035.parquet', 'source-document00154.parquet', 'source-document00222.parquet', 'source-document00381.parquet', 'source-document00432.parquet', 'source-document00034.parquet'], 'suspicious-document00044.parquet': ['source-document00182.parquet', 'source-document00357.parquet', 'source-document00140.parquet', 'source-document00395.parquet', 'source-document00141.parquet', 'source-document00394.parquet', 'source-document00419.parquet', 'source-document00022.parquet', 'source-document00355.parquet', 'source-document00180.parquet', 'source-document00425.parquet', 'source-document00234.parquet', 'source-document00168.parquet', 'source-document00382.parquet', 'source-document00157.parquet', 'source-document00009.parquet', 'source-document00036.parquet', 'source-document00195.parquet', 'source-document00431.parquet', 'source-document00037.parquet', 'source-document00194.parquet', 'source-document00433.parquet', 'source-document00035.parquet', 'source-document00154.parquet', 'source-document00222.parquet', 'source-document00432.parquet', 'source-document00034.parquet'], 'suspicious-document00252.parquet': ['source-document00357.parquet', 'source-document00425.parquet', 'source-document00154.parquet', 'source-document00222.parquet'], 'suspicious-document00078.parquet': ['source-document00357.parquet', 'source-document00009.parquet']}\n"
     ]
    }
   ],
   "source": [
    "print(suspicious_to_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Filter sentences based on potential plagiarisms\n",
    "printt(\"Filtering sentences...\")\n",
    "df_sentences_detections = sentence_filtering(cleaned_suspicious_path, cleaned_source_path, suspicious_to_sources)\n",
    "df_sentences_detections.to_parquet(\"df_sentences_detections.parquet\")\n",
    "printt(\"Divide Dataframe Sentences...\")\n",
    "divide_df_sentences(current_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Compute and save similarity scores between suspicious and source sentences\n",
    "printt(\"Processing similarity calculations...\")\n",
    "output_dir = os.path.join(current_dir, \"sentences-detections\")\n",
    "df_similarity = similarity_computation(\n",
    "    cleaned_suspicious_path, cleaned_source_path, output_dir, BETA\n",
    ")\n",
    "df_similarity.to_parquet(\"df_similarity.parquet\")\n",
    "for THRESHOLD in THRESHOLD_LIST:\n",
    "    printt(\n",
    "        f\"SOURCE_LIMIT: {SOURCE_LIMIT}, SUSPICIOUS_LIMIT: {SUSPICIOUS_LIMIT}, THRESHOLD: {THRESHOLD}, BETA: {BETA}\"\n",
    "    )\n",
    "    df_plagiarism_detections = df_similarity[\n",
    "        df_similarity[\"hybrid_similarity\"] > THRESHOLD\n",
    "    ]\n",
    "    # 8. Analysis of detections\n",
    "    printt(\"Evaluate detections...\")\n",
    "    precision, recall, f1_score, plagdet = evaluate_detection(\n",
    "        df_plagiarism_references, df_plagiarism_detections\n",
    "    )\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1_score:.4f}\")\n",
    "    print(f\"Plagdet-Score: {plagdet:.4f}\")\n",
    "    # 9. Save results\n",
    "    printt(f\"Saving results to {results_filename}\")\n",
    "    with open(results_filename, \"a\") as f:\n",
    "        f.write(\n",
    "            f\"{SUSPICIOUS_LIMIT}, {THRESHOLD}, {BETA}, {precision}, {recall}, {f1_score}, {plagdet}\\n\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Replication of Figure 20\n",
    "    print(\"Replication of Figure 20\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    reload_sources = True\n",
    "    reload_suspicious = True\n",
    "\n",
    "    results_filename = \"data_for_figure_20.txt\"\n",
    "    SOURCE_LIMIT = 64\n",
    "    SUSPICIOUS_LIMIT = 8\n",
    "    THRESHOLD_LIST = [0.65] # [0.4, 0.45, 0.5, 0.55, 0.60, 0.65, 0.70]\n",
    "    BETA = 0.5\n",
    "\n",
    "    # 1. Initial setup (paths and number of processes for parallelization)\n",
    "    printt(\"Starting the algorithm...\")\n",
    "\n",
    "    # Getting the current directory path and setting up the base path for data.\n",
    "    current_dir = os.getcwd()\n",
    "    base_data_path = os.path.join(\n",
    "        current_dir, \"pan-plagiarism-corpus-2011/external-detection-corpus\"\n",
    "    )\n",
    "\n",
    "    # Print the number of processes that will be used for parallel operations.\n",
    "    printt(\"N_PROCESSES:\", N_PROCESSES)\n",
    "\n",
    "    # 2. Directory setup\n",
    "    printt(\"Setting up directories...\")\n",
    "\n",
    "    # Initialize the directories for raw and cleaned versions of both source and suspicious documents.\n",
    "    # Set up paths for raw and cleaned source documents.\n",
    "    cleaned_source_path = os.path.join(current_dir, \"cleaned-source-documents\")\n",
    "    source_path = os.path.join(base_data_path, \"source-document\")\n",
    "\n",
    "    # Set up paths for raw and cleaned suspicious documents.\n",
    "    cleaned_suspicious_path = os.path.join(current_dir, \"cleaned-suspicious-documents\")\n",
    "    suspicious_path = os.path.join(base_data_path, \"suspicious-document\")\n",
    "\n",
    "    if reload_sources:\n",
    "        ensure_directory_exists_and_is_empty(cleaned_source_path)\n",
    "    if reload_suspicious:\n",
    "        ensure_directory_exists_and_is_empty(cleaned_suspicious_path)\n",
    "\n",
    "    # 3. Extract real results from XML files to understand the ground truth of plagiarism references\n",
    "    printt(\"Extracting reference results...\")\n",
    "\n",
    "    # Extract and store the plagiarism references from the suspicious XML files\n",
    "    df_plagiarism_references = extract_sources_from_suspicious_xml(suspicious_path)\n",
    "\n",
    "    # Step 4: Preprocess the raw data to prepare it for the subsequent analysis\n",
    "    printt(\"Preprocessing data...\")\n",
    "    printt(\"Source documents preprocessing...\")\n",
    "    if reload_sources:\n",
    "        preprocessing_data(source_path, cleaned_source_path, SOURCE_LIMIT)\n",
    "\n",
    "    printt(\"Suspicious documents preprocessing...\")\n",
    "    if reload_suspicious:\n",
    "        preprocessing_data(suspicious_path, cleaned_suspicious_path, SUSPICIOUS_LIMIT)\n",
    "\n",
    "    # Step 5: Identify potential source documents for each suspicious document\n",
    "    printt(\"Filtering documents...\")\n",
    "    suspicious_to_sources = document_filtering(cleaned_suspicious_path, cleaned_source_path)\n",
    "\n",
    "    # Step 6: Filter sentences based on potential plagiarisms\n",
    "    printt(\"Filtering sentences...\")\n",
    "    df_sentences_detections = sentence_filtering(cleaned_suspicious_path, cleaned_source_path, suspicious_to_sources)\n",
    "    df_sentences_detections.to_parquet(\"df_sentences_detections.parquet\")\n",
    "\n",
    "    printt(\"Divide Dataframe Sentences...\")\n",
    "    divide_df_sentences(current_dir)\n",
    "\n",
    "    # Step 7: Compute and save similarity scores between suspicious and source sentences\n",
    "    printt(\"Processing similarity calculations...\")\n",
    "    output_dir = os.path.join(current_dir, \"sentences-detections\")\n",
    "    df_similarity = similarity_computation(\n",
    "        cleaned_suspicious_path, cleaned_source_path, output_dir, BETA\n",
    "    )\n",
    "\n",
    "    df_similarity.to_parquet(\"df_similarity.parquet\")\n",
    "\n",
    "    for THRESHOLD in THRESHOLD_LIST:\n",
    "        printt(\n",
    "            f\"SOURCE_LIMIT: {SOURCE_LIMIT}, SUSPICIOUS_LIMIT: {SUSPICIOUS_LIMIT}, THRESHOLD: {THRESHOLD}, BETA: {BETA}\"\n",
    "        )\n",
    "\n",
    "        df_plagiarism_detections = df_similarity[\n",
    "            df_similarity[\"hybrid_similarity\"] > THRESHOLD\n",
    "        ]\n",
    "\n",
    "        # 8. Analysis of detections\n",
    "        printt(\"Evaluate detections...\")\n",
    "        precision, recall, f1_score, plagdet = evaluate_detection(\n",
    "            df_plagiarism_references, df_plagiarism_detections\n",
    "        )\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1_score:.4f}\")\n",
    "        print(f\"Plagdet-Score: {plagdet:.4f}\")\n",
    "\n",
    "        # 9. Save results\n",
    "        printt(f\"Saving results to {results_filename}\")\n",
    "        with open(results_filename, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{SUSPICIOUS_LIMIT}, {THRESHOLD}, {BETA}, {precision}, {recall}, {f1_score}, {plagdet}\\n\"\n",
    "            )\n",
    "\n",
    "    end_time = time.time()\n",
    "    diff_time = end_time - start_time\n",
    "    print(f\"Time taken for Figure 20 replication: {diff_time:.2f} seconds\")\n",
    "\n",
    "    print_sucess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Replication of Figure 20\n",
    "    print(\"Replication of Figure 20\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    reload_sources = True\n",
    "    reload_suspicious = True\n",
    "\n",
    "    results_filename = \"data_for_figure_20.txt\"\n",
    "    SOURCE_LIMIT = 64\n",
    "    SUSPICIOUS_LIMIT = 8\n",
    "    THRESHOLD_LIST = [0.65] # [0.4, 0.45, 0.5, 0.55, 0.60, 0.65, 0.70]\n",
    "    BETA = 0.5\n",
    "\n",
    "    # 1. Initial setup (paths and number of processes for parallelization)\n",
    "    printt(\"Starting the algorithm...\")\n",
    "\n",
    "    # Getting the current directory path and setting up the base path for data.\n",
    "    current_dir = os.getcwd()\n",
    "    base_data_path = os.path.join(\n",
    "        current_dir, \"pan-plagiarism-corpus-2011/external-detection-corpus\"\n",
    "    )\n",
    "\n",
    "    # Print the number of processes that will be used for parallel operations.\n",
    "    printt(\"N_PROCESSES:\", N_PROCESSES)\n",
    "\n",
    "    # 2. Directory setup\n",
    "    printt(\"Setting up directories...\")\n",
    "\n",
    "    # Initialize the directories for raw and cleaned versions of both source and suspicious documents.\n",
    "    # Set up paths for raw and cleaned source documents.\n",
    "    cleaned_source_path = os.path.join(current_dir, \"cleaned-source-documents\")\n",
    "    source_path = os.path.join(base_data_path, \"source-document\")\n",
    "\n",
    "    # Set up paths for raw and cleaned suspicious documents.\n",
    "    cleaned_suspicious_path = os.path.join(current_dir, \"cleaned-suspicious-documents\")\n",
    "    suspicious_path = os.path.join(base_data_path, \"suspicious-document\")\n",
    "\n",
    "    if reload_sources:\n",
    "        ensure_directory_exists_and_is_empty(cleaned_source_path)\n",
    "    if reload_suspicious:\n",
    "        ensure_directory_exists_and_is_empty(cleaned_suspicious_path)\n",
    "\n",
    "    # 3. Extract real results from XML files to understand the ground truth of plagiarism references\n",
    "    printt(\"Extracting reference results...\")\n",
    "\n",
    "    # Extract and store the plagiarism references from the suspicious XML files\n",
    "    df_plagiarism_references = extract_sources_from_suspicious_xml(suspicious_path)\n",
    "\n",
    "    # Step 4: Preprocess the raw data to prepare it for the subsequent analysis\n",
    "    printt(\"Preprocessing data...\")\n",
    "    printt(\"Source documents preprocessing...\")\n",
    "    if reload_sources:\n",
    "        preprocessing_data(source_path, cleaned_source_path, SOURCE_LIMIT)\n",
    "\n",
    "    printt(\"Suspicious documents preprocessing...\")\n",
    "    if reload_suspicious:\n",
    "        preprocessing_data(suspicious_path, cleaned_suspicious_path, SUSPICIOUS_LIMIT)\n",
    "\n",
    "    # Step 5: Identify potential source documents for each suspicious document\n",
    "    printt(\"Filtering documents...\")\n",
    "    suspicious_to_sources = document_filtering(cleaned_suspicious_path, cleaned_source_path)\n",
    "\n",
    "    # Step 6: Filter sentences based on potential plagiarisms\n",
    "    printt(\"Filtering sentences...\")\n",
    "    df_sentences_detections = sentence_filtering(cleaned_suspicious_path, cleaned_source_path, suspicious_to_sources)\n",
    "    df_sentences_detections.to_parquet(\"df_sentences_detections.parquet\")\n",
    "\n",
    "    printt(\"Divide Dataframe Sentences...\")\n",
    "    divide_df_sentences(current_dir)\n",
    "\n",
    "    # Step 7: Compute and save similarity scores between suspicious and source sentences\n",
    "    printt(\"Processing similarity calculations...\")\n",
    "    output_dir = os.path.join(current_dir, \"sentences-detections\")\n",
    "    df_similarity = similarity_computation(\n",
    "        cleaned_suspicious_path, cleaned_source_path, output_dir, BETA\n",
    "    )\n",
    "\n",
    "    df_similarity.to_parquet(\"df_similarity.parquet\")\n",
    "\n",
    "    for THRESHOLD in THRESHOLD_LIST:\n",
    "        printt(\n",
    "            f\"SOURCE_LIMIT: {SOURCE_LIMIT}, SUSPICIOUS_LIMIT: {SUSPICIOUS_LIMIT}, THRESHOLD: {THRESHOLD}, BETA: {BETA}\"\n",
    "        )\n",
    "\n",
    "        df_plagiarism_detections = df_similarity[\n",
    "            df_similarity[\"hybrid_similarity\"] > THRESHOLD\n",
    "        ]\n",
    "\n",
    "        # 8. Analysis of detections\n",
    "        printt(\"Evaluate detections...\")\n",
    "        precision, recall, f1_score, plagdet = evaluate_detection(\n",
    "            df_plagiarism_references, df_plagiarism_detections\n",
    "        )\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1_score:.4f}\")\n",
    "        print(f\"Plagdet-Score: {plagdet:.4f}\")\n",
    "\n",
    "        # 9. Save results\n",
    "        printt(f\"Saving results to {results_filename}\")\n",
    "        with open(results_filename, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{SUSPICIOUS_LIMIT}, {THRESHOLD}, {BETA}, {precision}, {recall}, {f1_score}, {plagdet}\\n\"\n",
    "            )\n",
    "\n",
    "    end_time = time.time()\n",
    "    diff_time = end_time - start_time\n",
    "    print(f\"Time taken for Figure 20 replication: {diff_time:.2f} seconds\")\n",
    "\n",
    "    print_sucess()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
